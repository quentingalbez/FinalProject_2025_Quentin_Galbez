{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27af0057",
   "metadata": {},
   "source": [
    "TWO towers approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28996a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:09:31.510965: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-17 02:09:31.513871: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-17 02:09:31.520724: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747440571.531525   59268 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747440571.534701   59268 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747440571.544504   59268 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747440571.544513   59268 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747440571.544515   59268 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747440571.544516   59268 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-17 02:09:31.547810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.api.layers import Concatenate, Dense, Dropout, Input\n",
    "from keras.api.models import Model\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.saving import load_model\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "import sys\n",
    "sys.path.append('../data')\n",
    "import load_data\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a925635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "Cleaning data...\n",
      "Data cleaned.\n"
     ]
    }
   ],
   "source": [
    "small_matrix, big_matrix, item_categories, item_features, social_network, user_features, captions   = load_data.load_data()\n",
    "\n",
    "item_features = item_features_agg = item_features.groupby(\"video_id\").agg({\n",
    "    \"play_cnt\": \"sum\",\n",
    "    \"share_cnt\": \"sum\",\n",
    "    \"download_cnt\": \"sum\",\n",
    "    \"comment_cnt\": \"sum\",\n",
    "    \"upload_type\": \"first\",\n",
    "    \"author_id\": \"first\",\n",
    "    \"video_duration\": \"first\"\n",
    "})\n",
    "item_features[\"video_duration\"] = item_features[\"video_duration\"].fillna(item_features[\"video_duration\"].median())\n",
    "item_features = item_features.dropna()\n",
    "item_features = item_features.drop_duplicates()\n",
    "all_categories = set(cat for sublist in item_categories[\"feat\"] for cat in sublist)\n",
    "for cat in all_categories:\n",
    "    item_features[f'cat_{cat}'] = item_categories[\"feat\"].apply(lambda x: int(cat in x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6fe7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement two tower model\n",
    "\n",
    "\n",
    "\n",
    "# User tower\n",
    "user_input = Input(shape=(user_features.shape[1],), name='user_input')\n",
    "user_dense = Dense(128, activation='relu')(user_input)\n",
    "user_dense = Dropout(0.2)(user_dense)\n",
    "user_dense = Dense(64, activation='relu')(user_dense)\n",
    "user_dense = Dense(32, activation='relu')(user_dense)\n",
    "user_dense = Dropout(0.2)(user_dense)\n",
    "\n",
    "# Item tower\n",
    "item_input = Input(shape=(item_features.shape[1],), name='item_input')\n",
    "item_dense = Dense(128, activation='relu')(item_input)\n",
    "item_dense = Dropout(0.2)(item_dense)\n",
    "item_dense = Dense(64, activation='relu')(item_dense)\n",
    "item_dense = Dense(32, activation='relu')(item_dense)\n",
    "item_dense = Dropout(0.2)(item_dense)\n",
    "\n",
    "# Concatenate towers\n",
    "merged = Concatenate()([user_dense, item_dense])\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "output = Dense(1, activation='linear', name='output')(merged)\n",
    "\n",
    "model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "model.compile(optimizer=Adam(), loss='mse', metrics=['mae'])\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb19d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels of string columns\n",
    "item_features[\"upload_type\"] = LabelEncoder().fit_transform(item_features[\"upload_type\"])\n",
    "user_features[\"user_active_degree\"] = LabelEncoder().fit_transform(user_features[\"user_active_degree\"])\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "user_features = StandardScaler().fit_transform(user_features)\n",
    "item_features = StandardScaler().fit_transform(item_features)\n",
    "watch_ratios = StandardScaler().fit_transform(big_matrix[[\"watch_ratio\"]].values)\n",
    "\n",
    "# Split into train and test\n",
    "\n",
    "user_features_train = user_features[big_matrix[\"user_id\"]]\n",
    "item_features_train = item_features[big_matrix[\"video_id\"]]\n",
    "y_train = watch_ratios\n",
    "\n",
    "\n",
    "user_features_test = user_features[small_matrix[\"user_id\"]]\n",
    "item_features_test = item_features[small_matrix[\"video_id\"]]\n",
    "y_test = StandardScaler().fit_transform(small_matrix[[\"watch_ratio\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4cfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 2ms/step - loss: 0.9436 - mae: 0.3324 - val_loss: 0.9387 - val_mae: 0.2956 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2ms/step - loss: 0.9376 - mae: 0.3260 - val_loss: 0.9400 - val_mae: 0.3055 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2ms/step - loss: 0.9683 - mae: 0.3258 - val_loss: 0.9381 - val_mae: 0.3021 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 2ms/step - loss: 0.9288 - mae: 0.3253 - val_loss: 0.9374 - val_mae: 0.3028 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 2ms/step - loss: 0.9463 - mae: 0.3256 - val_loss: 0.9387 - val_mae: 0.2989 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 2ms/step - loss: 0.9543 - mae: 0.3253 - val_loss: 0.9372 - val_mae: 0.3053 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 2ms/step - loss: 0.9433 - mae: 0.3252 - val_loss: 0.9414 - val_mae: 0.3080 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2ms/step - loss: 0.9463 - mae: 0.3255 - val_loss: 0.9439 - val_mae: 0.3145 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m90339/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9523 - mae: 0.3254\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m90352/90352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2ms/step - loss: 0.9523 - mae: 0.3254 - val_loss: 0.9430 - val_mae: 0.3179 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=[user_features_train, item_features_train],\n",
    "    y=y_train,\n",
    "    validation_data=([user_features_test, item_features_test], y_test),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "model.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d357d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m140456/140456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 463us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "two_tower_model = load_model(\"two-towers.keras\", custom_objects={'tf': tf})\n",
    "\n",
    "inverse_scaler = StandardScaler().fit(big_matrix[[\"watch_ratio\"]].values)\n",
    "\n",
    "y_true = inverse_scaler.inverse_transform(y_test).flatten()\n",
    "\n",
    "predictions = model.predict([user_features_test, item_features_test])\n",
    "y_pred = inverse_scaler.inverse_transform(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c21103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the model\n",
      "True values: [0.71234272 2.17924934 2.37223467 0.51962865 0.33643166]\n",
      "Predicted values: [[1.4275737 ]\n",
      " [1.4261898 ]\n",
      " [0.90504426]\n",
      " [1.2559516 ]\n",
      " [0.9100992 ]]\n",
      "MAE: 0.5125399806729959\n",
      "RMSE: 1.6252481923620308\n",
      "R2: 0.06278182109012642\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = root_mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Evaluate the model\")\n",
    "print(f\"True values: {y_true[:5]}\")\n",
    "print(f\"Predicted values: {y_pred[:5]}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbf6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion for the Two Towers Model\n",
    "print(\"Conclusion for the Two Towers Model:\")\n",
    "print(f\"The Two Towers model was trained to predict user-video interactions using user and item features.\")\n",
    "print(f\"Evaluation Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "\n",
    "if r2 > 0.5:\n",
    "    print(\"The model demonstrates good predictive performance.\")\n",
    "elif r2 > 0.2:\n",
    "    print(\"The model shows moderate predictive performance.\")\n",
    "else:\n",
    "    print(\"The model's predictive performance is limited, and further optimization may be needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed3d05",
   "metadata": {},
   "source": [
    "## Conclusion for the Two Towers Model\n",
    "\n",
    "The Two Towers model was designed to predict user-video interactions by leveraging both user and item features. The model was trained and evaluated using a dataset of user interactions with videos.\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **Mean Absolute Error (MAE):** 0.5125\n",
    "    - This metric indicates the average absolute difference between the predicted and actual values. A lower value suggests better performance.\n",
    "- **Root Mean Squared Error (RMSE):** 1.6252\n",
    "    - RMSE measures the square root of the average squared differences between predictions and actual values. It penalizes larger errors more heavily than MAE.\n",
    "- **R-squared (R2):** 0.0628\n",
    "    - R2 represents the proportion of variance in the dependent variable that is predictable from the independent variables. A low R2 score indicates that the model explains only a small portion of the variance in the data.\n",
    "\n",
    "### Insights:\n",
    "- The low R2 score suggests that the model's predictive performance is limited and that it struggles to capture the underlying patterns in the data.\n",
    "- The MAE and RMSE values provide a measure of the prediction errors, with RMSE emphasizing larger errors.\n",
    "\n",
    "\n",
    "\n",
    "### Final Thoughts:\n",
    "While the Two Towers model demonstrates some ability to predict user-video interactions, its current performance suggests significant room for improvement. Future iterations should focus on addressing the identified limitations through model refinement, feature engineering, and alternative approaches. With these enhancements, the model's predictive accuracy and generalization capabilities can be improved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
