{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee407e45",
   "metadata": {},
   "source": [
    "### Understanding ALS (Alternating Least Squares)\n",
    "\n",
    "ALS (Alternating Least Squares) is a collaborative filtering algorithm widely used in recommendation systems. It is particularly effective for matrix factorization tasks, where the goal is to predict missing values in a user-item interaction matrix. The algorithm is based on the principle of decomposing the interaction matrix into two lower-dimensional matrices: one representing user preferences and the other representing item features.\n",
    "\n",
    "#### How ALS Works:\n",
    "1. **Matrix Factorization**:\n",
    "    - Given a user-item interaction matrix `R` (e.g., ratings, watch ratios, etc.), ALS approximates it as the product of two matrices:\n",
    "      - `U`: A matrix where each row corresponds to a user and represents their latent preferences.\n",
    "      - `V`: A matrix where each row corresponds to an item and represents its latent features.\n",
    "    - The goal is to minimize the reconstruction error\n",
    "\n",
    "2. **Alternating Optimization**:\n",
    "    - ALS alternates between fixing one matrix (e.g., `U`) and solving for the other (e.g., `V`), and vice versa. This iterative process continues until convergence or a maximum number of iterations is reached.\n",
    "\n",
    "3. **Handling Sparsity**:\n",
    "    - Real-world interaction matrices are often sparse (i.e., most entries are missing). ALS efficiently handles this sparsity by only considering observed interactions during optimization.\n",
    "\n",
    "#### Key Features of ALS:\n",
    "1. **Scalability**:\n",
    "    - ALS is computationally efficient and can scale to large datasets, making it suitable for big data applications.\n",
    "\n",
    "2. **Cold-Start Handling**:\n",
    "    - ALS can handle missing data using strategies like `coldStartStrategy=\"drop\"`, which excludes users or items with insufficient data from predictions.\n",
    "\n",
    "3. **Regularization**:\n",
    "    - Regularization is applied to prevent overfitting, ensuring that the model generalizes well to unseen data.\n",
    "\n",
    "4. **Parallelization**:\n",
    "    - ALS is implemented in distributed frameworks like Apache Spark, enabling parallel computation across large clusters.\n",
    "\n",
    "#### Limitations of ALS:\n",
    "1. **Cold-Start Problem**:\n",
    "    - ALS struggles with new users or items that lack interaction data.\n",
    "2. **Linear Assumptions**:\n",
    "    - The algorithm assumes linear relationships between latent factors, which may not capture complex patterns in the data.\n",
    "\n",
    "Despite its limitations, ALS remains a powerful and widely used algorithm for collaborative filtering tasks, offering a balance between simplicity, scalability, and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9581b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "import sys\n",
    "sys.path.append('../data')\n",
    "import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb0df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "Cleaning data...\n",
      "Data cleaned.\n"
     ]
    }
   ],
   "source": [
    "small_matrix, big_matrix, item_categories, item_features, social_network, user_features, captions   = load_data.load_data()\n",
    "small_matrix.drop(columns=[\"play_duration\", \"video_duration\", \"time\", \"date\", \"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0efe9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 03:02:04 WARN Utils: Your hostname, pcfixe resolves to a loopback address: 127.0.1.1; using 192.168.1.3 instead (on interface enp5s0)\n",
      "25/05/17 03:02:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/17 03:02:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Setup Spark.\n",
    "spark = SparkSession.builder.appName(\"ALS\").getOrCreate()\n",
    "spark_df = spark.createDataFrame(small_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c95df5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALS_85b33fd4371a"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als = ALS(maxIter=15, regParam=0.01, userCol=\"user_id\", itemCol=\"video_id\", ratingCol=\"watch_ratio\", coldStartStrategy=\"drop\")\n",
    "als.setSeed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7ea430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 03:02:59 WARN TaskSetManager: Stage 0 contains a task of very large size (2961 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/17 03:03:00 WARN TaskSetManager: Stage 1 contains a task of very large size (2961 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/17 03:03:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/05/17 03:03:08 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "25/05/17 03:03:08 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/05/17 03:03:08 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "25/05/17 03:03:09 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/05/17 03:03:09 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "25/05/17 03:03:09 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "25/05/17 03:03:09 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "25/05/17 03:03:09 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(spark_df)\n",
    "\n",
    "model_path = \"modeld_als_sav\"\n",
    "\n",
    "if os.path.exists(model_path) and os.path.isdir(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "\n",
    "model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da362ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|     14|[{5210, 3.3426616...|\n",
      "|     19|[{9178, 2.511615}...|\n",
      "|     21|[{9178, 2.8554924...|\n",
      "|     23|[{5365, 3.2164779...|\n",
      "|     24|[{7383, 2.4718285...|\n",
      "|     36|[{9178, 2.476555}...|\n",
      "|     37|[{9178, 2.9262342...|\n",
      "|     41|[{9178, 2.697055}...|\n",
      "|     51|[{9178, 3.1278403...|\n",
      "|     55|[{9815, 3.145819}...|\n",
      "|     64|[{9178, 3.500213}...|\n",
      "|     73|[{9815, 3.4126804...|\n",
      "|     75|[{9178, 2.1038976...|\n",
      "|     97|[{9178, 2.691554}...|\n",
      "|     98|[{9178, 2.5094604...|\n",
      "|    102|[{9178, 2.2952542...|\n",
      "|    120|[{9178, 2.1472466...|\n",
      "|    127|[{9178, 2.082949}...|\n",
      "|    129|[{9178, 3.5886247...|\n",
      "|    131|[{6523, 6.544776}...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 03:18:13 WARN TaskSetManager: Stage 646 contains a task of very large size (2961 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------+\n",
      "|user_id|recommendations                                                                               |\n",
      "+-------+----------------------------------------------------------------------------------------------+\n",
      "|14     |[{5210, 3.3426616}, {9178, 3.1054316}, {7383, 2.9617243}, {4040, 2.867227}, {8298, 2.8307033}]|\n",
      "|19     |[{9178, 2.511615}, {7383, 2.4508276}, {4040, 2.4472446}, {314, 2.368181}, {8298, 2.3449864}]  |\n",
      "|21     |[{9178, 2.8554924}, {4040, 2.788542}, {7383, 2.7624385}, {314, 2.7037168}, {8298, 2.6912348}] |\n",
      "|23     |[{5365, 3.2164779}, {9178, 3.005998}, {4040, 2.9460378}, {7383, 2.9212122}, {314, 2.8971171}] |\n",
      "|24     |[{7383, 2.4718285}, {4040, 2.4301996}, {9178, 2.4197152}, {314, 2.3857317}, {8298, 2.33107}]  |\n",
      "|36     |[{9178, 2.476555}, {4040, 2.3870633}, {7383, 2.3789985}, {314, 2.312498}, {8298, 2.2989814}]  |\n",
      "|37     |[{9178, 2.9262342}, {7383, 2.886976}, {4040, 2.8667872}, {314, 2.817174}, {8298, 2.7840407}]  |\n",
      "|41     |[{9178, 2.697055}, {4040, 2.6461685}, {7383, 2.5910878}, {8298, 2.5382211}, {314, 2.5374374}] |\n",
      "|51     |[{9178, 3.1278403}, {4040, 3.042214}, {7383, 3.0307686}, {314, 2.9166155}, {8298, 2.8957064}] |\n",
      "|55     |[{9815, 3.145819}, {2586, 2.941177}, {5365, 2.7273946}, {9178, 2.6773171}, {314, 2.6526003}]  |\n",
      "|64     |[{9178, 3.500213}, {7383, 3.368553}, {4040, 3.3265529}, {314, 3.2585225}, {8298, 3.242512}]   |\n",
      "|73     |[{9815, 3.4126804}, {721, 3.135598}, {2586, 3.081741}, {7383, 2.9451733}, {4040, 2.924879}]   |\n",
      "|75     |[{9178, 2.1038976}, {7383, 2.0300088}, {4040, 2.0122182}, {314, 1.9661623}, {8298, 1.950936}] |\n",
      "|97     |[{9178, 2.691554}, {7383, 2.579974}, {4040, 2.5397112}, {314, 2.4844246}, {8298, 2.4612315}]  |\n",
      "|98     |[{9178, 2.5094604}, {4040, 2.4212964}, {7383, 2.4166405}, {314, 2.3440228}, {8298, 2.339726}] |\n",
      "|102    |[{9178, 2.2952542}, {4040, 2.2487054}, {7383, 2.2316017}, {314, 2.164665}, {10500, 2.1398165}]|\n",
      "|120    |[{9178, 2.1472466}, {7383, 2.0773175}, {4040, 2.0543084}, {314, 2.0274282}, {8298, 1.9930917}]|\n",
      "|127    |[{9178, 2.082949}, {7383, 2.0075357}, {4040, 1.9874276}, {8298, 1.936671}, {314, 1.9350874}]  |\n",
      "|129    |[{9178, 3.5886247}, {4040, 3.5037088}, {7383, 3.470378}, {8298, 3.3930576}, {314, 3.345101}]  |\n",
      "|131    |[{6523, 6.544776}, {5626, 4.255184}, {5365, 4.248284}, {9178, 2.978361}, {4040, 2.9116921}]   |\n",
      "+-------+----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "# Load the model from disk\n",
    "loaded_model = ALSModel.load(model_path)\n",
    "\n",
    "# Generate a top-N ranked list of recommendations for each user\n",
    "n = 10\n",
    "\n",
    "user_recs = loaded_model.recommendForAllUsers(n)\n",
    "user_recs.show()\n",
    "\n",
    "top_recommendations = loaded_model.recommendForUserSubset(spark_df.select(\"user_id\").distinct(), n)\n",
    "top_recommendations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b20615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 03:09:34 WARN TaskSetManager: Stage 611 contains a task of very large size (2961 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/17 03:09:36 WARN TaskSetManager: Stage 615 contains a task of very large size (2961 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/17 03:09:37 WARN TaskSetManager: Stage 619 contains a task of very large size (2961 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 619:>                                                      (0 + 24) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model evaluation:\n",
      "Root-mean-square error = 1.1829326735324064\n",
      "Mean absolute error = 0.35043772919713145\n",
      "R2 = 0.23951807584652962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = loaded_model.transform(spark_df)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"watch_ratio\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\n\\nModel evaluation:\")\n",
    "\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "print(\"Mean absolute error = \" + str(mae))\n",
    "print(\"R2 = \" + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda67fb",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The ALS (Alternating Least Squares) model was successfully implemented to generate video recommendations based on user interaction data. The model was trained on a subset of the data (`small_matrix`) and evaluated using standard regression metrics such as RMSE, MAE, and R². The results of the evaluation are as follows:\n",
    "\n",
    "- **Root Mean Square Error (RMSE):** 1.1829  \n",
    "    This metric indicates the average magnitude of error between the predicted and actual watch ratios. A lower RMSE value is desirable, but the current value suggests there is room for improvement in prediction accuracy.\n",
    "\n",
    "- **Mean Absolute Error (MAE):** 0.3504  \n",
    "    This metric measures the average absolute difference between the predicted and actual watch ratios. The relatively low MAE indicates that the model performs reasonably well in capturing user preferences.\n",
    "\n",
    "- **R² Score:** 0.2395  \n",
    "    The R² score measures the proportion of variance in the dependent variable (watch ratio) that is predictable from the independent variables. A score of 0.2395 suggests that the model explains only a small portion of the variance, indicating potential for further optimization.\n",
    "\n",
    "The model was also used to generate top-N recommendations for users, providing personalized video suggestions. These recommendations can be leveraged to enhance user engagement and satisfaction by tailoring content to individual preferences.\n",
    "\n",
    "#### Key Observations:\n",
    "1. The model's performance metrics indicate that while it provides a good starting point, there is significant scope for improvement in terms of accuracy and generalization.\n",
    "2. The cold-start strategy (`drop`) was employed to handle missing data, ensuring that the model does not generate predictions for users or items with insufficient data.\n",
    "\n",
    "#### Final Thoughts:\n",
    "The ALS model provides a solid foundation for building a recommendation system tailored to user preferences. With further refinement and experimentation, it has the potential to deliver highly accurate and personalized recommendations, driving user engagement and satisfaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
